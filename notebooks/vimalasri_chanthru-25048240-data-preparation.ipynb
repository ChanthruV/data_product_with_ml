{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pandas and numpy packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\chant\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\data-product-with-ml-ZVsQeeWq-py3.9\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Define the root folder where your zip files are located\n",
    "root_folder = r'C:\\Users\\chant\\OneDrive\\Desktop\\itineraries_csv'\n",
    "\n",
    "# Define the destination folder where you want to extract the CSVs\n",
    "destination_folder = r'C:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\data\\raw'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Iterate through the subdirectories in the root folder\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            zip_file_path = os.path.join(subdir, file)\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the CSV files are located\n",
    "directory = r'C:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\data\\raw'\n",
    "\n",
    "# Initialize an empty list to store DataFrames from individual CSV files\n",
    "dfs = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now, combined_df contains all the data from the CSV files in a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\notebooks\\vimalasri_chanthru-25048240-data-preparation.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chant/adv_mla_2023/data_product_with_ml/notebooks/vimalasri_chanthru-25048240-data-preparation.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_csv_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mchant\u001b[39m\u001b[39m\\\u001b[39m\u001b[39madv_mla_2023\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata_product_with_ml\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39minterim\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcombined_data.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chant/adv_mla_2023/data_product_with_ml/notebooks/vimalasri_chanthru-25048240-data-preparation.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Save the combined DataFrame to a CSV file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chant/adv_mla_2023/data_product_with_ml/notebooks/vimalasri_chanthru-25048240-data-preparation.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m combined_df\u001b[39m.\u001b[39mto_csv(output_csv_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chant/adv_mla_2023/data_product_with_ml/notebooks/vimalasri_chanthru-25048240-data-preparation.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Print a message to confirm that the CSV file has been saved\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chant/adv_mla_2023/data_product_with_ml/notebooks/vimalasri_chanthru-25048240-data-preparation.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCombined DataFrame saved to \u001b[39m\u001b[39m{\u001b[39;00moutput_csv_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path where to save the combined DataFrame as a CSV file\n",
    "output_csv_path = r'C:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\data\\interim\\combined_data.csv'\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Print a message to confirm that the CSV file has been saved\n",
    "print(f\"Combined DataFrame saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = r'C:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\data\\interim\\combined_data.csv'\n",
    "combined_df = pd.read_csv(output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13519999, 23)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>legId</th>\n",
       "      <th>searchDate</th>\n",
       "      <th>flightDate</th>\n",
       "      <th>startingAirport</th>\n",
       "      <th>destinationAirport</th>\n",
       "      <th>travelDuration</th>\n",
       "      <th>isBasicEconomy</th>\n",
       "      <th>isRefundable</th>\n",
       "      <th>isNonStop</th>\n",
       "      <th>totalFare</th>\n",
       "      <th>totalTravelDistance</th>\n",
       "      <th>segmentsDepartureTimeEpochSeconds</th>\n",
       "      <th>segmentsDepartureTimeRaw</th>\n",
       "      <th>segmentsArrivalTimeEpochSeconds</th>\n",
       "      <th>segmentsArrivalTimeRaw</th>\n",
       "      <th>segmentsArrivalAirportCode</th>\n",
       "      <th>segmentsDepartureAirportCode</th>\n",
       "      <th>segmentsAirlineName</th>\n",
       "      <th>segmentsAirlineCode</th>\n",
       "      <th>segmentsEquipmentDescription</th>\n",
       "      <th>segmentsDurationInSeconds</th>\n",
       "      <th>segmentsDistance</th>\n",
       "      <th>segmentsCabinCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ca0e81111c683bec1012473feefd28f</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H29M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.60</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1650214620</td>\n",
       "      <td>2022-04-17T12:57:00.000-04:00</td>\n",
       "      <td>1650223560</td>\n",
       "      <td>2022-04-17T15:26:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>8940</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98685953630e772a098941b71906592b</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H30M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.60</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1650191400</td>\n",
       "      <td>2022-04-17T06:30:00.000-04:00</td>\n",
       "      <td>1650200400</td>\n",
       "      <td>2022-04-17T09:00:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9000</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98d90cbc32bfbb05c2fc32897c7c1087</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H30M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.60</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1650209700</td>\n",
       "      <td>2022-04-17T11:35:00.000-04:00</td>\n",
       "      <td>1650218700</td>\n",
       "      <td>2022-04-17T14:05:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 757-200</td>\n",
       "      <td>9000</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>969a269d38eae583f455486fa90877b4</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H32M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.60</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1650218340</td>\n",
       "      <td>2022-04-17T13:59:00.000-04:00</td>\n",
       "      <td>1650227460</td>\n",
       "      <td>2022-04-17T16:31:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9120</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980370cf27c89b40d2833a1d5afc9751</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H34M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.60</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1650203940</td>\n",
       "      <td>2022-04-17T09:59:00.000-04:00</td>\n",
       "      <td>1650213180</td>\n",
       "      <td>2022-04-17T12:33:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9240</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>b8632c8d6306eefa042de33dd303fc21</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>PT2H30M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>168.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1650247020</td>\n",
       "      <td>2022-04-17T21:57:00.000-04:00</td>\n",
       "      <td>1650256020</td>\n",
       "      <td>2022-04-17T23:27:00.000-05:00</td>\n",
       "      <td>DFW</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Spirit Airlines</td>\n",
       "      <td>NK</td>\n",
       "      <td>AIRBUS INDUSTRIE A321 SHARKLETS</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>a5e3ac25a2a23b16e9a7c82eb3dbe5c6</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>PT2H13M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>202.60</td>\n",
       "      <td>725.0</td>\n",
       "      <td>1650198600</td>\n",
       "      <td>2022-04-17T08:30:00.000-04:00</td>\n",
       "      <td>1650206580</td>\n",
       "      <td>2022-04-17T09:43:00.000-05:00</td>\n",
       "      <td>DAL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 717</td>\n",
       "      <td>7980</td>\n",
       "      <td>725</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>46cab91070ddf01f23ad6d59600c2bff</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>PT2H15M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>202.60</td>\n",
       "      <td>725.0</td>\n",
       "      <td>1650227940</td>\n",
       "      <td>2022-04-17T16:39:00.000-04:00</td>\n",
       "      <td>1650236040</td>\n",
       "      <td>2022-04-17T17:54:00.000-05:00</td>\n",
       "      <td>DAL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 717</td>\n",
       "      <td>8100</td>\n",
       "      <td>725</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>b8227b6368a7bc1c8d83591695af18cb</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>PT2H16M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>202.60</td>\n",
       "      <td>725.0</td>\n",
       "      <td>1650221820</td>\n",
       "      <td>2022-04-17T14:57:00.000-04:00</td>\n",
       "      <td>1650229980</td>\n",
       "      <td>2022-04-17T16:13:00.000-05:00</td>\n",
       "      <td>DAL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 717</td>\n",
       "      <td>8160</td>\n",
       "      <td>725</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2bcf2f66e9c0d933d452ab9941adf829</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>PT2H16M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>202.60</td>\n",
       "      <td>725.0</td>\n",
       "      <td>1650206580</td>\n",
       "      <td>2022-04-17T10:43:00.000-04:00</td>\n",
       "      <td>1650214740</td>\n",
       "      <td>2022-04-17T11:59:00.000-05:00</td>\n",
       "      <td>DAL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 717</td>\n",
       "      <td>8160</td>\n",
       "      <td>725</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               legId  searchDate  flightDate startingAirport   \n",
       "0   9ca0e81111c683bec1012473feefd28f  2022-04-16  2022-04-17             ATL  \\\n",
       "1   98685953630e772a098941b71906592b  2022-04-16  2022-04-17             ATL   \n",
       "2   98d90cbc32bfbb05c2fc32897c7c1087  2022-04-16  2022-04-17             ATL   \n",
       "3   969a269d38eae583f455486fa90877b4  2022-04-16  2022-04-17             ATL   \n",
       "4   980370cf27c89b40d2833a1d5afc9751  2022-04-16  2022-04-17             ATL   \n",
       "..                               ...         ...         ...             ...   \n",
       "95  b8632c8d6306eefa042de33dd303fc21  2022-04-16  2022-04-17             ATL   \n",
       "96  a5e3ac25a2a23b16e9a7c82eb3dbe5c6  2022-04-16  2022-04-17             ATL   \n",
       "97  46cab91070ddf01f23ad6d59600c2bff  2022-04-16  2022-04-17             ATL   \n",
       "98  b8227b6368a7bc1c8d83591695af18cb  2022-04-16  2022-04-17             ATL   \n",
       "99  2bcf2f66e9c0d933d452ab9941adf829  2022-04-16  2022-04-17             ATL   \n",
       "\n",
       "   destinationAirport travelDuration  isBasicEconomy  isRefundable  isNonStop   \n",
       "0                 BOS        PT2H29M           False         False       True  \\\n",
       "1                 BOS        PT2H30M           False         False       True   \n",
       "2                 BOS        PT2H30M           False         False       True   \n",
       "3                 BOS        PT2H32M           False         False       True   \n",
       "4                 BOS        PT2H34M           False         False       True   \n",
       "..                ...            ...             ...           ...        ...   \n",
       "95                DFW        PT2H30M           False         False       True   \n",
       "96                DFW        PT2H13M           False         False       True   \n",
       "97                DFW        PT2H15M           False         False       True   \n",
       "98                DFW        PT2H16M           False         False       True   \n",
       "99                DFW        PT2H16M           False         False       True   \n",
       "\n",
       "    totalFare  totalTravelDistance segmentsDepartureTimeEpochSeconds   \n",
       "0      248.60                947.0                        1650214620  \\\n",
       "1      248.60                947.0                        1650191400   \n",
       "2      248.60                947.0                        1650209700   \n",
       "3      248.60                947.0                        1650218340   \n",
       "4      248.60                947.0                        1650203940   \n",
       "..        ...                  ...                               ...   \n",
       "95     168.59                  NaN                        1650247020   \n",
       "96     202.60                725.0                        1650198600   \n",
       "97     202.60                725.0                        1650227940   \n",
       "98     202.60                725.0                        1650221820   \n",
       "99     202.60                725.0                        1650206580   \n",
       "\n",
       "         segmentsDepartureTimeRaw segmentsArrivalTimeEpochSeconds   \n",
       "0   2022-04-17T12:57:00.000-04:00                      1650223560  \\\n",
       "1   2022-04-17T06:30:00.000-04:00                      1650200400   \n",
       "2   2022-04-17T11:35:00.000-04:00                      1650218700   \n",
       "3   2022-04-17T13:59:00.000-04:00                      1650227460   \n",
       "4   2022-04-17T09:59:00.000-04:00                      1650213180   \n",
       "..                            ...                             ...   \n",
       "95  2022-04-17T21:57:00.000-04:00                      1650256020   \n",
       "96  2022-04-17T08:30:00.000-04:00                      1650206580   \n",
       "97  2022-04-17T16:39:00.000-04:00                      1650236040   \n",
       "98  2022-04-17T14:57:00.000-04:00                      1650229980   \n",
       "99  2022-04-17T10:43:00.000-04:00                      1650214740   \n",
       "\n",
       "           segmentsArrivalTimeRaw segmentsArrivalAirportCode   \n",
       "0   2022-04-17T15:26:00.000-04:00                        BOS  \\\n",
       "1   2022-04-17T09:00:00.000-04:00                        BOS   \n",
       "2   2022-04-17T14:05:00.000-04:00                        BOS   \n",
       "3   2022-04-17T16:31:00.000-04:00                        BOS   \n",
       "4   2022-04-17T12:33:00.000-04:00                        BOS   \n",
       "..                            ...                        ...   \n",
       "95  2022-04-17T23:27:00.000-05:00                        DFW   \n",
       "96  2022-04-17T09:43:00.000-05:00                        DAL   \n",
       "97  2022-04-17T17:54:00.000-05:00                        DAL   \n",
       "98  2022-04-17T16:13:00.000-05:00                        DAL   \n",
       "99  2022-04-17T11:59:00.000-05:00                        DAL   \n",
       "\n",
       "   segmentsDepartureAirportCode segmentsAirlineName segmentsAirlineCode   \n",
       "0                           ATL               Delta                  DL  \\\n",
       "1                           ATL               Delta                  DL   \n",
       "2                           ATL               Delta                  DL   \n",
       "3                           ATL               Delta                  DL   \n",
       "4                           ATL               Delta                  DL   \n",
       "..                          ...                 ...                 ...   \n",
       "95                          ATL     Spirit Airlines                  NK   \n",
       "96                          ATL               Delta                  DL   \n",
       "97                          ATL               Delta                  DL   \n",
       "98                          ATL               Delta                  DL   \n",
       "99                          ATL               Delta                  DL   \n",
       "\n",
       "       segmentsEquipmentDescription segmentsDurationInSeconds   \n",
       "0                       Airbus A321                      8940  \\\n",
       "1                       Airbus A321                      9000   \n",
       "2                    Boeing 757-200                      9000   \n",
       "3                       Airbus A321                      9120   \n",
       "4                       Airbus A321                      9240   \n",
       "..                              ...                       ...   \n",
       "95  AIRBUS INDUSTRIE A321 SHARKLETS                      9000   \n",
       "96                       Boeing 717                      7980   \n",
       "97                       Boeing 717                      8100   \n",
       "98                       Boeing 717                      8160   \n",
       "99                       Boeing 717                      8160   \n",
       "\n",
       "   segmentsDistance segmentsCabinCode  \n",
       "0               947             coach  \n",
       "1               947             coach  \n",
       "2               947             coach  \n",
       "3               947             coach  \n",
       "4               947             coach  \n",
       "..              ...               ...  \n",
       "95              NaN             coach  \n",
       "96              725             coach  \n",
       "97              725             coach  \n",
       "98              725             coach  \n",
       "99              725             coach  \n",
       "\n",
       "[100 rows x 23 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of data and all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13519999 entries, 0 to 13519998\n",
      "Data columns (total 23 columns):\n",
      " #   Column                             Dtype  \n",
      "---  ------                             -----  \n",
      " 0   legId                              object \n",
      " 1   searchDate                         object \n",
      " 2   flightDate                         object \n",
      " 3   startingAirport                    object \n",
      " 4   destinationAirport                 object \n",
      " 5   travelDuration                     object \n",
      " 6   isBasicEconomy                     bool   \n",
      " 7   isRefundable                       bool   \n",
      " 8   isNonStop                          bool   \n",
      " 9   totalFare                          float64\n",
      " 10  totalTravelDistance                float64\n",
      " 11  segmentsDepartureTimeEpochSeconds  object \n",
      " 12  segmentsDepartureTimeRaw           object \n",
      " 13  segmentsArrivalTimeEpochSeconds    object \n",
      " 14  segmentsArrivalTimeRaw             object \n",
      " 15  segmentsArrivalAirportCode         object \n",
      " 16  segmentsDepartureAirportCode       object \n",
      " 17  segmentsAirlineName                object \n",
      " 18  segmentsAirlineCode                object \n",
      " 19  segmentsEquipmentDescription       object \n",
      " 20  segmentsDurationInSeconds          object \n",
      " 21  segmentsDistance                   object \n",
      " 22  segmentsCabinCode                  object \n",
      "dtypes: bool(3), float64(2), object(18)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "# Display the summary of columns\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points of investigation: \n",
    "\n",
    "- dtype: object\n",
    "- Investigate missing values, potential negative values and duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalFare</th>\n",
       "      <th>totalTravelDistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.352000e+07</td>\n",
       "      <td>1.256038e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.737535e+02</td>\n",
       "      <td>1.569619e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.075496e+02</td>\n",
       "      <td>8.414888e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.397000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.235800e+02</td>\n",
       "      <td>8.620000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.446000e+02</td>\n",
       "      <td>1.392000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.876000e+02</td>\n",
       "      <td>2.376000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.260610e+03</td>\n",
       "      <td>4.430000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          totalFare  totalTravelDistance\n",
       "count  1.352000e+07         1.256038e+07\n",
       "mean   3.737535e+02         1.569619e+03\n",
       "std    2.075496e+02         8.414888e+02\n",
       "min    2.397000e+01         9.700000e+01\n",
       "25%    2.235800e+02         8.620000e+02\n",
       "50%    3.446000e+02         1.392000e+03\n",
       "75%    4.876000e+02         2.376000e+03\n",
       "max    8.260610e+03         4.430000e+03"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the descriptive statistics\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Observations: no negative values for totalFare (our target) and totalTravelDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legId',\n",
       " 'searchDate',\n",
       " 'flightDate',\n",
       " 'startingAirport',\n",
       " 'destinationAirport',\n",
       " 'travelDuration',\n",
       " 'isBasicEconomy',\n",
       " 'isRefundable',\n",
       " 'isNonStop',\n",
       " 'totalFare',\n",
       " 'totalTravelDistance',\n",
       " 'segmentsDepartureTimeEpochSeconds',\n",
       " 'segmentsDepartureTimeRaw',\n",
       " 'segmentsArrivalTimeEpochSeconds',\n",
       " 'segmentsArrivalTimeRaw',\n",
       " 'segmentsArrivalAirportCode',\n",
       " 'segmentsDepartureAirportCode',\n",
       " 'segmentsAirlineName',\n",
       " 'segmentsAirlineCode',\n",
       " 'segmentsEquipmentDescription',\n",
       " 'segmentsDurationInSeconds',\n",
       " 'segmentsDistance',\n",
       " 'segmentsCabinCode']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of columns before cleaning \n",
    "\n",
    "columns_before = combined_df.columns.tolist()\n",
    "\n",
    "columns_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'combined_df' is your DataFrame and it's already been loaded with data.\n",
    "unique_starting_airports = combined_df['startingAirport'].unique()\n",
    "unique_destination_airports = combined_df['destinationAirport'].unique()\n",
    "unique_segments_AirlineName = combined_df['segmentsAirlineName'].unique()\n",
    "\n",
    "# Now 'unique_starting_airports' and 'unique_destination_airports' are arrays containing the unique values\n",
    "# from their respective columns in 'combined_df'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOS', 'CLT', 'DEN', 'DFW', 'DTW', 'EWR', 'IAD', 'JFK', 'LAX',\n",
       "       'LGA', 'MIA', 'OAK', 'ORD', 'PHL', 'SFO', 'ATL'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_destination_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ATL', 'BOS', 'CLT', 'DEN', 'DFW', 'DTW', 'EWR', 'IAD', 'JFK',\n",
       "       'LAX', 'LGA', 'MIA', 'OAK', 'ORD', 'PHL', 'SFO'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_starting_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Delta', 'JetBlue Airways', 'American Airlines||American Airlines',\n",
       "       'United||United', 'Spirit Airlines||Spirit Airlines',\n",
       "       'American Airlines', 'JetBlue Airways||JetBlue Airways',\n",
       "       'Frontier Airlines', 'United',\n",
       "       'Frontier Airlines||Frontier Airlines', 'Spirit Airlines',\n",
       "       'Delta||Delta', 'Delta||United',\n",
       "       'Delta||Cape Air||Cape Air||Delta', 'Delta||Delta||Delta',\n",
       "       'Delta||Cape Air||Cape Air||United', 'United||Delta',\n",
       "       'United||United||Delta',\n",
       "       'Frontier Airlines||Frontier Airlines||Frontier Airlines',\n",
       "       'Alaska Airlines',\n",
       "       'Spirit Airlines||Spirit Airlines||Spirit Airlines',\n",
       "       'Delta||United||Delta', 'United||United||Alaska Airlines',\n",
       "       'Alaska Airlines||Alaska Airlines',\n",
       "       'American Airlines||American Airlines||American Airlines',\n",
       "       'Delta||Alaska Airlines', 'United||Delta||Delta',\n",
       "       'United||Alaska Airlines', 'United||United||United',\n",
       "       'United||United||United||Delta',\n",
       "       'United||United||Alaska Airlines||Alaska Airlines',\n",
       "       'United||Alaska Airlines||Alaska Airlines',\n",
       "       'Alaska Airlines||Alaska Airlines||Alaska Airlines',\n",
       "       'Delta||Alaska Airlines||United',\n",
       "       'JetBlue Airways||JetBlue Airways||JetBlue Airways',\n",
       "       'JetBlue Airways||Cape Air||Cape Air', 'Delta||United||United',\n",
       "       'Delta||Alaska Airlines||Alaska Airlines',\n",
       "       'Alaska Airlines||United', 'Boutique Air||Boutique Air||United',\n",
       "       'Alaska Airlines||Alaska Airlines||United',\n",
       "       'American Airlines||Key Lime Air||Key Lime Air',\n",
       "       'American Airlines||American Airlines||Cape Air||Cape Air',\n",
       "       'United||Key Lime Air||Key Lime Air',\n",
       "       'Delta||Alaska Airlines||Delta',\n",
       "       'Delta||United||United||Alaska Airlines',\n",
       "       'United||Cape Air||Cape Air||Delta', 'Delta||Delta||United',\n",
       "       'Southern Airways Express||Southern Airways Express||American Airlines',\n",
       "       'Delta||Delta||Alaska Airlines', 'United||United||United||United',\n",
       "       'Boutique Air||Boutique Air||American Airlines',\n",
       "       'Sun Country Airlines||Sun Country Airlines',\n",
       "       'Cape Air||Cape Air||Delta', 'Cape Air||Cape Air',\n",
       "       'Alaska Airlines||Delta',\n",
       "       'Alaska Airlines||Alaska Airlines||Delta',\n",
       "       'Alaska Airlines||United||Delta',\n",
       "       'Cape Air||Cape Air||Cape Air||Delta',\n",
       "       'Cape Air||Cape Air||Cape Air',\n",
       "       'Cape Air||JetBlue Airways||JetBlue Airways',\n",
       "       'Cape Air||Cape Air||JetBlue Airways', 'JetBlue Airways||Cape Air',\n",
       "       'Cape Air||United', 'Hawaiian Airlines||Hawaiian Airlines',\n",
       "       'Cape Air||Delta', 'United||Alaska Airlines||United',\n",
       "       'American Airlines||Cape Air||Cape Air||American Airlines',\n",
       "       'American Airlines||Southern Airways Express||Southern Airways Express',\n",
       "       'American Airlines||Boutique Air||Boutique Air',\n",
       "       'American Airlines||Contour Airlines||Contour Airlines',\n",
       "       'American Airlines||Cape Air||Cape Air',\n",
       "       'United||United||Delta||Delta',\n",
       "       'United||Cape Air||Cape Air||United',\n",
       "       'Key Lime Air||Key Lime Air||American Airlines',\n",
       "       'Key Lime Air||Key Lime Air||United',\n",
       "       'United||Delta||Alaska Airlines', 'United||Alaska Airlines||Delta',\n",
       "       'Contour Airlines||Contour Airlines||American Airlines',\n",
       "       'Key Lime Air||Key Lime Air',\n",
       "       'Alaska Airlines||United||Alaska Airlines',\n",
       "       'Delta||United||Alaska Airlines',\n",
       "       'JetBlue Airways||Cape Air||JetBlue Airways',\n",
       "       'United||Southern Airways Express||Southern Airways Express',\n",
       "       'JetBlue Airways||American Airlines', 'United||Cape Air',\n",
       "       'Delta||Cape Air||Cape Air||Cape Air',\n",
       "       'United||Cape Air||Cape Air||Cape Air',\n",
       "       'Southern Airways Express||Southern Airways Express||United',\n",
       "       'Cape Air||JetBlue Airways',\n",
       "       'Cape Air||Cape Air||Cape Air||United',\n",
       "       'Cape Air||Cape Air||Cape Air||JetBlue Airways',\n",
       "       'Delta||United||United||United', 'Alaska Airlines||Delta||Delta',\n",
       "       'Alaska Airlines||United||United', 'Delta||Cape Air',\n",
       "       'Alaska Airlines||Delta||United',\n",
       "       'Southern Airways Express||Southern Airways Express||Alaska Airlines',\n",
       "       'Delta||Delta||United||United',\n",
       "       'American Airlines||JetBlue Airways',\n",
       "       'United||United||United||Alaska Airlines',\n",
       "       'Alaska Airlines||United||United||United',\n",
       "       'Alaska Airlines||Alaska Airlines||United||United',\n",
       "       'Delta||United||United||Delta',\n",
       "       'Hawaiian Airlines||Alaska Airlines', 'Hawaiian Airlines||United',\n",
       "       'United||Boutique Air||Boutique Air',\n",
       "       'Cape Air||Cape Air||American Airlines||American Airlines',\n",
       "       'Cape Air||Cape Air||Delta||Delta',\n",
       "       'Cape Air||Cape Air||American Airlines',\n",
       "       'Cape Air||Cape Air||United||United',\n",
       "       'Cape Air||Cape Air||Alaska Airlines',\n",
       "       'Cape Air||Cape Air||Alaska Airlines||Alaska Airlines',\n",
       "       'JetBlue Airways||JetBlue Airways||Cape Air',\n",
       "       'United||Cape Air||Cape Air||Alaska Airlines',\n",
       "       'United||United||Alaska Airlines||Delta',\n",
       "       'Alaska Airlines||Southern Airways Express||Southern Airways Express',\n",
       "       'United||Hawaiian Airlines'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_segments_AirlineName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of data and save it into a variable data_cleaned\n",
    "data_cleaned=combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary columns - given the business problem and the required columns necessary for deployment via the streamlit app, we can remove a number of columns. \n",
    "\n",
    "columns_to_drop = [\n",
    "    'segmentsDepartureTimeEpochSeconds',\n",
    "    'segmentsDepartureTimeRaw',\n",
    "    'segmentsArrivalTimeEpochSeconds',\n",
    "    'segmentsArrivalTimeRaw',\n",
    "    'segmentsArrivalAirportCode',\n",
    "    'segmentsDepartureAirportCode',\n",
    "    'segmentsAirlineCode',\n",
    "    'segmentsEquipmentDescription',\n",
    "    'segmentsDurationInSeconds',\n",
    "    'segmentsDistance'\n",
    "]\n",
    "\n",
    "data_cleaned = data_cleaned.drop(columns=columns_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legId                       0\n",
       "searchDate                  0\n",
       "flightDate                  0\n",
       "startingAirport             0\n",
       "destinationAirport          0\n",
       "travelDuration              0\n",
       "isBasicEconomy              0\n",
       "isRefundable                0\n",
       "isNonStop                   0\n",
       "totalFare                   0\n",
       "totalTravelDistance    959619\n",
       "segmentsAirlineName         0\n",
       "segmentsCabinCode           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm there are any missing values  \n",
    "data_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values for totalTravelDistance. Due to the nature of this field, will impute it with its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in 'totalTravelDistance' with the mean of the column\n",
    "data_cleaned['totalTravelDistance'].fillna(data_cleaned['totalTravelDistance'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legId                  0\n",
       "searchDate             0\n",
       "flightDate             0\n",
       "startingAirport        0\n",
       "destinationAirport     0\n",
       "travelDuration         0\n",
       "isBasicEconomy         0\n",
       "isRefundable           0\n",
       "isNonStop              0\n",
       "totalFare              0\n",
       "totalTravelDistance    0\n",
       "segmentsAirlineName    0\n",
       "segmentsCabinCode      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm there are no missing values remaining \n",
    "\n",
    "data_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_columns = data_cleaned.columns[data_cleaned.isnull().any()]\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows and remove them\n",
    "duplicate_rows = data_cleaned.duplicated()\n",
    "if duplicate_rows.any():\n",
    "    print(f\"Number of duplicate rows: {sum(duplicate_rows)}\")\n",
    "    data_cleaned = data_cleaned[~duplicate_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the column 'sales_revenue' and save it into variable called target\n",
    "target=data_cleaned.pop('totalFare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    PT2H29M\n",
       "1    PT2H30M\n",
       "2    PT2H30M\n",
       "3    PT2H32M\n",
       "4    PT2H34M\n",
       "Name: travelDuration, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned['travelDuration'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain data for Streamlit app \n",
    "\n",
    "import re\n",
    "\n",
    "# Define the duration_to_seconds function\n",
    "def duration_to_seconds(duration_str):\n",
    "    match = re.match(r'PT(\\d+)H(\\d+)M', duration_str)\n",
    "    if match:\n",
    "        hours = int(match.group(1))\n",
    "        minutes = int(match.group(2))\n",
    "        total_seconds = hours * 3600 + minutes * 60\n",
    "        return total_seconds\n",
    "    else:\n",
    "        # Handle invalid duration format (assign a default value, e.g., 0 seconds)\n",
    "        return 0\n",
    "\n",
    "# Assuming 'travelDuration' is a column in your data_cleaned DataFrame\n",
    "data_cleaned['travelDuration'] = data_cleaned['travelDuration'].apply(duration_to_seconds)\n",
    "\n",
    "# Now, the 'travelDuration' column in data_cleaned is in seconds, and invalid formats are set to 0 seconds\n",
    "\n",
    "#Extract data needed for the streamlit app\n",
    "\n",
    "selected_columns = ['startingAirport', 'destinationAirport', 'totalTravelDistance', 'travelDuration']\n",
    "extracted_data = data_cleaned[selected_columns]\n",
    "\n",
    "# Specify the path to save the CSV file\n",
    "output_path = r'C:\\Users\\chant\\adv_mla_2023\\data_product_with_ml\\data\\processed\\extracted_data.csv'\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "extracted_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Extracted data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique values for 'legId': 1721518\n",
      "Count of unique values for 'searchDate': 32\n",
      "Count of unique values for 'flightDate': 92\n",
      "Count of unique values for 'startingAirport': 16\n",
      "Count of unique values for 'destinationAirport': 16\n",
      "Count of unique values for 'isBasicEconomy': 2\n",
      "Count of unique values for 'isRefundable': 2\n",
      "Count of unique values for 'isNonStop': 2\n",
      "Count of unique values for 'segmentsAirlineName': 118\n",
      "Count of unique values for 'segmentsCabinCode': 52\n"
     ]
    }
   ],
   "source": [
    "#Handling Categorical Variables - determine number of unique categories for each of these features\n",
    "\n",
    "# Convert NumPy array to DataFrame\n",
    "data_cleaned_df = pd.DataFrame(data_cleaned, columns=data_cleaned.columns)\n",
    "\n",
    "# Select columns with dtype 'object' or 'bool'\n",
    "object_columns = data_cleaned_df.select_dtypes(include=['object', 'bool'])\n",
    "\n",
    "# Find the count of unique values for each object column\n",
    "unique_values_count_dict = {}\n",
    "for column in object_columns:\n",
    "    unique_values_count = data_cleaned_df[column].nunique()\n",
    "    unique_values_count_dict[column] = unique_values_count\n",
    "\n",
    "# Print count of unique values for each object column\n",
    "for column, count in unique_values_count_dict.items():\n",
    "    print(f\"Count of unique values for '{column}': {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 'legId'. 'searchdate', 'flightdate' can be transformed into datetime objects and then numeric.  The other time-related columns are duration based and can be converted to numeric.\n",
    "\n",
    "The remaining variables are categorical nominal so label encoding is not ideal. A number of features have high cardinality so we will use hashing encoding for all of them for simplicity. This is especially benficial given the large size of the dataframe which makes one-hot encoding sub-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.drop('legId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to numeric (0 or 1)\n",
    "data_cleaned[['isBasicEconomy', 'isRefundable', 'isNonStop']] = data_cleaned[['isBasicEconomy', 'isRefundable', 'isNonStop']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'searchdate' and 'flightDate' into standard datetime columns then numeric for ML \n",
    "data_cleaned[['searchDate', 'flightDate']] = data_cleaned[['searchDate', 'flightDate']].apply(lambda x: pd.to_datetime(x).astype('int64') // 10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hashing Encoding \n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Specify the columns for which you want to apply hashing encoding\n",
    "columns_to_hash = [\n",
    "    'startingAirport',\n",
    "    'destinationAirport',\n",
    "    'segmentsAirlineName',\n",
    "    'segmentsCabinCode',\n",
    "    'travelDuration'\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize the FeatureHasher\n",
    "hasher = FeatureHasher(n_features=5, input_type='string')  # Choose the number of bins (n_features)\n",
    "\n",
    "# Prepare an iterable of iterables of strings for hashing encoding\n",
    "hashed_features = []\n",
    "for row in data_cleaned[columns_to_hash].values:\n",
    "    hashed_row = [str(value) for value in row]\n",
    "    hashed_features.append(hashed_row)\n",
    "\n",
    "# Apply hashing encoding to the specified columns\n",
    "hashed_features = hasher.transform(hashed_features)\n",
    "\n",
    "# Convert hashed_features to a DataFrame\n",
    "hashed_df = pd.DataFrame(hashed_features.toarray())\n",
    "\n",
    "# Specify columns to drop\n",
    "columns_to_drop = columns_to_hash\n",
    "\n",
    "# Concatenate the hashed DataFrame with the original data and remove specified columns\n",
    "data_cleaned = pd.concat([data_cleaned.drop(columns=columns_to_drop), hashed_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13519999 entries, 0 to 13519998\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   searchDate           int64  \n",
      " 1   flightDate           int64  \n",
      " 2   isBasicEconomy       int32  \n",
      " 3   isRefundable         int32  \n",
      " 4   isNonStop            int32  \n",
      " 5   totalTravelDistance  float64\n",
      " 6   0                    float64\n",
      " 7   1                    float64\n",
      " 8   2                    float64\n",
      " 9   3                    float64\n",
      " 10  4                    float64\n",
      "dtypes: float64(6), int32(3), int64(2)\n",
      "memory usage: 979.9 MB\n"
     ]
    }
   ],
   "source": [
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['searchDate',\n",
       " 'flightDate',\n",
       " 'isBasicEconomy',\n",
       " 'isRefundable',\n",
       " 'isNonStop',\n",
       " 'totalTravelDistance',\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_after = data_cleaned.columns.tolist()\n",
    "\n",
    "columns_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns: ['legId', 'startingAirport', 'destinationAirport', 'travelDuration', 'totalFare', 'segmentsDepartureTimeEpochSeconds', 'segmentsDepartureTimeRaw', 'segmentsArrivalTimeEpochSeconds', 'segmentsArrivalTimeRaw', 'segmentsArrivalAirportCode', 'segmentsDepartureAirportCode', 'segmentsAirlineName', 'segmentsAirlineCode', 'segmentsEquipmentDescription', 'segmentsDurationInSeconds', 'segmentsDistance', 'segmentsCabinCode']\n"
     ]
    }
   ],
   "source": [
    "#Collate list of removed columns and save into list - if test_data becomes accesible, can be used to clean data more efficiently \n",
    "\n",
    "removed_columns = [column for column in columns_before if column not in columns_after]\n",
    "\n",
    "print(\"Removed columns:\", removed_columns)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the list using pickle\n",
    "with open('removed_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(removed_columns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13519999, 11)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>searchDate</th>\n",
       "      <th>flightDate</th>\n",
       "      <th>isBasicEconomy</th>\n",
       "      <th>isRefundable</th>\n",
       "      <th>isNonStop</th>\n",
       "      <th>totalTravelDistance</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1650067200</td>\n",
       "      <td>1650153600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1650067200</td>\n",
       "      <td>1650153600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1650067200</td>\n",
       "      <td>1650153600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1650067200</td>\n",
       "      <td>1650153600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1650067200</td>\n",
       "      <td>1650153600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   searchDate  flightDate  isBasicEconomy  isRefundable  isNonStop   \n",
       "0  1650067200  1650153600               0             0          1  \\\n",
       "1  1650067200  1650153600               0             0          1   \n",
       "2  1650067200  1650153600               0             0          1   \n",
       "3  1650067200  1650153600               0             0          1   \n",
       "4  1650067200  1650153600               0             0          1   \n",
       "\n",
       "   totalTravelDistance    0    1    2    3    4  \n",
       "0                947.0  1.0  0.0  0.0  1.0 -1.0  \n",
       "1                947.0  1.0  0.0  0.0  2.0  0.0  \n",
       "2                947.0  1.0  0.0  0.0  2.0  0.0  \n",
       "3                947.0  2.0  0.0  0.0  1.0  0.0  \n",
       "4                947.0  1.0  1.0  0.0  1.0  0.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searchDate             0\n",
       "flightDate             0\n",
       "isBasicEconomy         0\n",
       "isRefundable           0\n",
       "isNonStop              0\n",
       "totalTravelDistance    0\n",
       "0                      0\n",
       "1                      0\n",
       "2                      0\n",
       "3                      0\n",
       "4                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Split Train and Test Sets for Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import StandardScaler from sklearn.preprocessing and instantiate the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Import StandardScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_cleaned.columns = data_cleaned.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and apply the scaling on data_cleaned\n",
    "data_cleaned=scaler.fit_transform(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/scaler_dropped_fe.joblib']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dump from joblib and save the scaler into the folder models and call the file scaler.joblib\n",
    "from joblib import dump\n",
    "\n",
    "dump(scaler, '../models/scaler_dropped_fe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import function scaler_split_train_test from data.sets\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from src.data.sets import split_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the scaler data into training (80%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val=split_train_test(df=data_cleaned,target=target,test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the function save_sets from sets and save the sets into the folder data/processed\n",
    "from src.data.sets import save_sets\n",
    "save_sets(X_train, y_train, X_val, y_val, path='../data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the function load_sets from sets and load the sets from data/processed\n",
    "from src.data.sets import load_sets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_sets(path='../data/processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import statistics\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the mode of the target variable from the training set\n",
    "y_mode=mode(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a numpy array called y_base of dimensions (len(y_train), 1) filled with the mode value\n",
    "y_base=np.full((len(y_train),1),y_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Training: 44268.18510943663\n"
     ]
    }
   ],
   "source": [
    "# Import the function print_class_perf from models.performance and display the ROC-AUC score\n",
    "from src.models.performance import print_mse\n",
    "\n",
    "print_mse(y_train,y_base,set_name='Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5087c292a60e433539a3ea1db33a548e8b6d2b5fd0ec051734d3134be8f1ad1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARMAX & Elastic Net Regression\n",
    "\n",
    "Our goals with this experiment are to: \n",
    "\n",
    "1) Handling Multivariate Data:\n",
    "\n",
    "VARMA: It is specifically designed for multivariate time series data where the variables have interdependencies.\n",
    "Elastic Net: It can also handle multivariate regression problems, though it does not model the time series aspect.\n",
    "\n",
    "2) Explore Regularization and Feature Selection:\n",
    "\n",
    "VARMA: While VARMA itself does not include regularization, the VARMAX model, an extension that includes exogenous variables, can include L1 regularization (similar to Lasso) in some implementations.\n",
    "Elastic Net: It includes both L1 and L2 regularization, which helps in feature selection and in dealing with multicollinearity, respectively.\n",
    "\n",
    "Ideally we will be able to obtain results from VARMAX but it is computationally taxing. We will be able to gain similar benefits from Elastic Net Regression (minus the time series component) so will rely on that if not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chant\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-product-with-ml-ZVsQeeWq-py3.9\\lib\\site-packages\\statsmodels\\tsa\\statespace\\varmax.py:161: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.\n",
      "  warn('Estimation of VARMA(p,q) models is not generically robust,'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the saved sets from data/processed using numpy\n",
    "X_train = np.load('../data/processed/X_train.npy')\n",
    "X_val = np.load('../data/processed/X_val.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_val = np.load('../data/processed/y_val.npy')\n",
    "\n",
    "# Specify the fraction of data to use (e.g., 20%)\n",
    "data_fraction = 0.2\n",
    "\n",
    "# Randomly select a subset of the data for training\n",
    "X_train_subset, _, y_train_subset, _ = train_test_split(\n",
    "    X_train, y_train, train_size=data_fraction, random_state=42\n",
    ")\n",
    "\n",
    "# Create a pandas DataFrame from your training data\n",
    "feature_columns = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11']\n",
    "train_df = pd.DataFrame(X_train_subset, columns=feature_columns)\n",
    "\n",
    "# Create a pandas DataFrame for the target variable\n",
    "target_df = pd.DataFrame(y_train_subset, columns=['totalFare'])\n",
    "\n",
    "# Combine the target variable and feature variables\n",
    "train_df = pd.concat([target_df, train_df], axis=1)\n",
    "\n",
    "# Fit a VARMA model\n",
    "p = 2  # Specify the order for autoregressive (AR) component\n",
    "q = 1  # Specify the order for moving average (MA) component\n",
    "model = VARMAX(train_df, order=(p, q))\n",
    "model_fitted = model.fit(disp=False)\n",
    "\n",
    "# Save the fitted model to a .joblib file\n",
    "joblib.dump(model_fitted, '../models/varma_model.joblib')\n",
    "\n",
    "# Make predictions on validation data\n",
    "forecast = model_fitted.forecast(steps=len(X_val))\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for the predictions\n",
    "mse = mean_squared_error(y_val, forecast)\n",
    "\n",
    "print(f'Mean Squared Error (MSE) on Validation Data: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was left to run overnight - after 1000 mins, and despite training on only 20% of the data, no results were yielded. As expected, it was computationally taxing given the large dataset. We will utilise Elastic Net Regression instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on Training Data: 27711.33\n",
      "Mean Squared Error (MSE) on Validation Data: 27828.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load the saved sets from data/processed using numpy\n",
    "X_train = np.load('../data/processed/X_train.npy')\n",
    "X_val = np.load('../data/processed/X_val.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_val = np.load('../data/processed/y_val.npy')\n",
    "\n",
    "# Initialize the ElasticNet model\n",
    "elastic_net_model = ElasticNet(random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the fitted model to a .joblib file\n",
    "joblib.dump(elastic_net_model, '../models/elastic_net_model.joblib')\n",
    "\n",
    "# Make predictions on both training and validation data\n",
    "y_train_pred = elastic_net_model.predict(X_train)\n",
    "y_val_pred = elastic_net_model.predict(X_val)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for the predictions\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# Print MSE for training and validation data\n",
    "print(f'Mean Squared Error (MSE) on Training Data: {mse_train:.2f}')\n",
    "print(f'Mean Squared Error (MSE) on Validation Data: {mse_val:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs better than the base model but worse than Random Forest, XGBoost and Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-product-with-ml-ZVsQeeWq-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
